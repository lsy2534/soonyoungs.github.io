---
layout: post
title: Deep Learning
date: 2025-02-17 00:00 +0800
last_modified_at: 2025-02-17 00:08:25 +0800
tags: [AI, Deep LEarnig ]
categoie: [AI]
topic:  true

---
**Deep learning**!
{: .message }

1. 딥러닝은 머신러닝 알고리즘의 한 종류이다. 데이터 처리 유니(Unit)의 층(layer)을 여러 개 쌓아 구조적이지 않은 데이터에서 고수준 표현을 학습한다.

딥러닝으로 처리할수있는 비정형 데이터, 그 다음 분류 작업을 해결하기위해 데이터 처리 유닛을 여러 층으로 쌓는 메커니즘학습을 목표로 둔다.

# 개요  

머신러닝 알고르즘은 테이블 형태의 정형 데이터를 입력으로 사용합니다. 이 데이터에서는 각 샘플을 설명하는 특성이 열로 표현됩니다. 예를 들면 어떤 사람의 나이, 소득, 지난달 웹사이트 방문 횟수 등의 테이블 구조로 나타나게 된다. 이런 특성을 사용하여 이 사람이 다음 달에 특정 온라인 서비스를 구독할지 예측할수 있다(분석으로 활용하는 예)
이러한 테이블 구조를 사용하여 로지스틱 회귀 (랜덤 포레스트) XGBoost 모델을 훈련하여 이진 응답변수를 예측할수 있습니다.  

* 통계학에서는 출력을 '응답 변수(response variable)', '종속 변수(dependant variable) ' 등으로 부른다. 머신 러닝 분야에서는 '타깃(targit)' 또는 정답으로 많이들 부른다.

이사람이 구독을 할지(1)/ 구독을 안할지(0)으로 2진 표기가 가능

이러한 모델은 이런 특성이 출력에 어떻게 영향을 미치는지 학습합니다.

비정형 데이터는 이미지, 오디오, 텍스트와 같이 태생적으로 특성의 열로 구성할 수 없는 데이터들을 이야기한다.
비정형 데이터인 픽셀, 진동수, 문자 하나하나에는 정보가 거의 의미가 없다. 예를 들어 234번째 픽셀이 황토색이라는 사실은 이 이미지가 집인지 강아지인지 구별하는데 전혀 연관성이 없다.

 딥러닝을 정형 데이터에 적용할 수도 있지만, 딥러닝의 진정한 힘은 비정형 데이터를 다루는 능력이다. 새로운 이미지나 텍스트와 같은 비정형 데이터를 생성하는 것이 관심 대상이기 떄문에 딥러닝이 생성 모델 분야에 큰 영향을 줄고 있다.

 ### 1. 심층 신경망

여러 은닉 층(hidden Layer)을 쌓은 인공 신경망(ANN) 이런 이유로 딥러닝이 심층 신경망과 거의 동의어어가 되었다.

## 1.1 신경망
 - 심층 신경망은 층을 연속하여 쌓아 구성합니다. 층은 유닛을 가지며 이전 층의 유닛과 가중치로 연결 된다. 층의 모든 유닛이 이전 층의 모든 유닛과 연골되는 완전 연결 층 (또는 밀집 층)이 있다. 인접한 모든 층이 완전히 연결된 신경망을 다층 퍼셉트론(MLP)이라고 한다.  
 입력은 출력 층에 도달할 때까지 각 층에서 순서대로 변호나 됩니다다. 이를 네트워크를 관통하는 정방향 계산 이라고 부릅니다. 구체적으로 각 유닛은 입력의 가중치 합에 비선형 변환을 적용하고 이 출력을 다음 층으로 전달합니다. 최종 출력 층은 이 과정의 정점에 해당하며 하나의 유닛을 사용해 원래 입력이 특정 카테고리에 속할 확률을 출력한다.  
 - 각 층의 가중치 조합을 찾는 것에 있고 이러한 가중치를 찾는 과정을 네트워크를 훈련한다고 말한다.
 - 네트워크를 통해 거꾸로 전파되어 예측을 가장 많이 향상하는 방향으로 가중치를 조금씩 수정합니다. 이 과정을 역전파라고 부른다.  

## 1.2 고수준 특성 학습
 - 사람의 개입 없이도 입력 데이터에서 특성을 학습하는 능력이다. 후속 층의 유닛은 이전 층의 저수준 특성을 결합하여 원본 입력의 점점 더 정교한 측면을 표현 할 수 있습니다. 

 # 2. 다층 퍼셉트론(MLP)
## 학습 목표
 * 지도 학습(정답을 알려주고 이 답을 찾는 학습 과정) MLP를 훈련해보도록 한다.  
 알고리즘의 목표는 입력 데이터와 출력 레이블 사이의 매핑을 학습하여 이전에 본 적 없는 새로운 데이터에서 예측을 만드는 것입니다.
  
  MLP는 생성모델이 아닌 판별 모델이지만 여러 유형의 생성모델에서 지도 학습이 여전히 중요한 역할을 하므로 좋은 주제가 된다.

기본적으로 이미지 데이터는 각 픽셀 채넣에 대해서 0 ~ 255 사이의 정수로 구성 신경망은 각 입력의 절댓값이 1보다 작을 때 가장 잘 작동하므로 먼저 이미지를 전처리하여 이 정숫값을 0~1 사이로 만들어야 한다.  

신경망 출력은 이미지가 각 클래스에 속할 확률이기 떄문에 이미지의 정수 레이블을 원 핫 인코딩된(*카테고리형 데이터 를 숫자로 변환)  

### 2.1 데이터 준비
```
import numpy as np
import matplotlib.pyplot as plt

from tensorflow.keras import layers, models, optimizers, utils, datasets
from notebooks.utils import display

NUM_CLASSES = 10

(x_train, y_train), (x_test, y_test) = datasets.cifar10.load_data()

x_train = x_train.astype("float32") / 255.0
x_test = x_test.astype("float32") / 255.0

y_train = utils.to_categorical(y_train, NUM_CLASSES)
y_test = utils.to_categorical(y_test, NUM_CLASSES)

display(x_train[:10])
print(y_train[:10])
```

### 2.2 모델만들기

Sequential 모델은 일렬로 층(Layer)을 쌓은 네트워크를 빠르게 만들 때 사용하기 좋은 알고리즘이다. 가장 간단한 모델중 하나 이며, 입력 -> 처리 -> 출력 순서 대로 한 방향으로 만 흐르는 네트워크를 구성할때 사용
* 간단한 분류기 처리에는 적합한 모델
- 손글씨 숫자 인식
- 개/고양이 분류
- 스팸 이메일 탐지
- 질병 진단


함수형 API(Funtional API)를 사용하여 만든 동일한 MLP를 보여줍니다. 모델의 입력층과 출력 층을 정의합니다.

```
input_layer = layers.Input((32, 32, 3))

x = layers.Flatten()(input_layer)
x = layers.Dense(200, activation="relu")(x)
x = layers.Dense(150, activation="relu")(x)

output_layer = layers.Dense(NUM_CLASSES, activation="softmax")(x)

model = models.Model(input_layer, output_layer)

model.summary()
```

## 층(layers)
MLP를 만들 떄 세 종류의 층을 사용했다. Input, Flatten, Doense 층이다.


Input 층은 네트워크의 시적점입니다. 네트워크가 기대하는 입력 데이터 크기를 튜플로 알려주어야 한다. (32,32,3) 형식의 튜플로 크기를 지정, 배치의 크기는 지정하지 않는다(입력 데이터의 형태) Input 층에 임의의 이미지 개수를 전달할 수 있기 떄문에 배치 크기는 필요하지 않는다.

Flatten층은 입력을 하나의 벡터로 펼친다. 결과 벡터의 길이가 3,072(32, 32, 3)이다. 뒤따르던 Dense 층이 다차원 배열이 아니라 평평한 입력을 기대하기 떄문이다. 다른 종류의 층은 입력으로 다차원 배열을 사용해야한다. 

Dense 층은 기본적인 신경망 구성 요소이다. 이 층에서는 이전 층과 완전하게 연결 되는 유닛이 있다. 이 층의 각 유닛은 이전 층의 모든 유닛과 연결 된다. 유닛의 출력은 이전 층에서 받은 입력과 가중치를 곱하여 더한 것이다. 그다음 비선형 활성화 함수를 통과하여 다음 층으로 전달 된다. 활성화 함수는 신경망이 복잡한 함수 를 학습하는 데 아주 중요한 역할을 한다. 그렇지 않으면 입력을 선형적으로 조합한 값만 출력할 것이다.

## 활성화 함수(Activation Function)
활성화 함수는 신경망에서 뉴런이 출력할 값을 결정하는 함수
입력 데이터를 비선형적으로 변환하여 복잡한 패털을 학습할수 있도록 도와준다.


활성화 함수의 종류가 많으나 가장 대표적인 세 개의 활성화 함수는 렐루, 시그모이드, 소프트 맥스이다.

렐루(ReLU)는 가장 많이 사용되는 활성화 함수이며 양수 값 유지, 음수는 0으로 변환하여 계산이 간단하고 학습 속도가 빠른특징을 가진다. 리키렐루(Leaky ReLU) 활성화 함수는 한가지만 뺴고 렐루와 매우 비슷하다. 입력에 비례하는 작은 음수를 반환한다.

시그모이드 활성화 함수는 층의 출력을 0 ~ 1 사이로 조정하고 싶을 때 유용하다. 예를 들어 출력 유닛이 하나인 이진 분류 문제나 샘플이 하나 이상의 클래스에 속할 수 있는 다중 에리블 분류 문제에서 사용한다.

소프트 맥스 활성화 함수는 층의 전체 출력 합이 1이 되어야 할 때 사용합니다. 예를 들어 샘플이 정확히 하나의 클래스에만 속해야 하는 다중 분류 문제입니다.

### 모델 조사하기
model.summary() 메서드를 사용해 각 층의 크기를 조사할 수 있다.

심층 신경망을 CPU대신 GPU에서 훈련할 때 훈련 성능이 향상되는 이유기이도 하다.GPU는 텐서 곱셈에 최적화되어 있는데, 이런 계산이 복잡한 그래픽 조작에도 피룡하기 떄문이다.
Summary메서드는 각 층에서 훈련될 파리미터(가중치)의 수도 알려준다. 모델이 너무 느리게 훈련된다면 Summary 메서드를 확인해서 너무 많은 가중치가 있는 층윽 확인하고 이런층이 있다면 이층의 유닛의 개수를 중여서 훈련 속도를 높일 수 있을지 검토해야한다.


## 모델 컴파일
손실함수와 옵티마이저로 모델을 컴파일 해본다.

```
from tensorflow.keas import optimizers

opt = opimizers.Adam(learning_rate=0.0005)
model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['acciracy'])

```

### 손실함수
손실함수(loss function)는 신경망이 예측 출력과 정답을 비교하는 데 사용합니다. 이 함수는 샘플마다 하나의 수치를 반환한다. 이 값이 클수록 이 샘플에 대한 네트워크의 수행 결과가 좋지 않다는 뜻이다.
케라스는 많은 손실 함수를 기본으로 제공하며 자신만의 손실 함수를 정의할 수도 있습니다. 가장 많이 사용하는 세 개의 손실 함수는 평평균 제곱 오차, 범주형 크로스 엔트로피와 이진 크로스 엔트로피 입다.

신경망이 회귀(연속적인 값을 예측)을 풀기 위해 설계 되었다면 평균 제곱 오차 손실(Mean Squared Error)을 사용한다. 

평균 제곱 오차 손실(MES)를 사용하는 이유  
1. 오차 크기를 강조
 * 오차를 제곱하기 떄문에 큰 오차일수록 더 큰 패널티를 받음 -> 큰 오차를 줄이려는 방향으로 모델이 학습 됨.
2. 미분 가능하여 최적화에 용이
 * MES는 미분이 가능하므로 경사 하강법 등을 이용한 최적화가 가능함

 여러 클래스 중에 하나에 속해야 하는 분류 문제라면 범주형 크로스 엔트로피가 알 맞은 손실 함수이다.

 마지막으로 출력 유닛이 하나인 이진 분류 문제이거나 샘플이 여러 클래스에 속할 수 있는 다중 레이블 분류 문제라면 이진 크로스 엔트로피를 사용해야한다.


### 옵티마저
옵티마이저는 실손 함수의 그레이디언트를 기반으로 신경망의 가중치를 업데이트 할때 사용하는 알고리즘이다. 널리 사용하고 안정적인 옵티마이저 중 하나는 Adam이다. 학습률을 제외하면 일반적으로 Adam 옵티마이저의 기본 매개변수를 바꿀 필요가 없다 학습률이 클수록 한 번의 훈련 스텝에서 가중치를 크게 바꿉ㄴ다 학습률이 크면 초기에 훈련 속도가 빠르지만 훈련이 조금 불안정한 단점이 있고 손실 함수의 전역 최솟값을 찾지 못할수도 있다. 이 매개변수는 훈련 과정에서 튜닝하고 조정해야한다.

널리 사용하는 다흔 옵티마이저는 RMSProp입니다 이 옵티마이저의 매개변수도 조정할 필요가 없습니다. 하지만 매개변수의 역할을 이해해두면 좋을꺼다.

모델의 compile 메서드에 손실 함수와 옵티마이저를 전달한다. metrics 매개변수에는 정확도 같이 훈련 과정에서 기록하고 싶은 지표를 추가로 지정할 수 있다.

### 2.3 모델 훈련하기
앞서 모델에 어떤 데이터도 전달하지 않았다. 네트워크의 구조를 정의 하고 구조를 정의하고 모델에 손실 함수와 옵티마이저를 연결했다.

데이터로 모델을 훈련하려면 fit메소드를 호출한다.


```
model.fit(x_train,
          y_train,
          batch_size = 32, 
          epochs = 10,
          shuffle = True
)
```